{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality Summary\n",
    "In this notebook, we did\n",
    "* Baseline experiments for TFIDF \n",
    "* built the whole vocabulary on the RFA and testing on publications belonging to the test data\n",
    "* all evaluations are done on the test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import argparse \n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#local \n",
    "import utils_bsl as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the order that they will be used\n",
    "parser = argparse.ArgumentParser(description = 'TFIDF for Grant recommendation')\n",
    "parser.add_argument('-data_path', type = str, default = 'newdata/', \n",
    "                    help = 'complete path to the training data [default:newdata/]')\n",
    "parser.add_argument('-load_pretrained', type = bool, default = False,\n",
    "                    help = 'whether to load pretrained TFIDF embeddings & corpus & vectorizer [default:False]')\n",
    "parser.add_argument('-load_path', type = str, default = 'evalAuto/tfidf/', \n",
    "                    help = 'path where TFIDF embeddings & corpus & vectorizers are saved [default:evalAuto/tfidf/]')\n",
    "# some training parameters regardinf TFIDF \n",
    "parser.add_argument('-ngram_range', type = str, default = '(1,2)', help = 'see sklearn TFIDF params')\n",
    "parser.add_argument('-min_df', type = int, default = 2, help = 'see sklearn TFIDF params')\n",
    "parser.add_argument('-max_features', type = int, default = 2000, help = 'see sklearn TFIDF params')\n",
    "parser.add_argument('-top', type = int, default = 10, \n",
    "                    help = 'number of recommendations to take [default:10]')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    seed_val = 1234\n",
    "    ut.set_seed(seed_val) \n",
    "    \n",
    "    # get logger started\n",
    "    logging.basicConfig(level=logging.ERROR, filename= args.load_path + \"logfile\", filemode=\"a+\",\n",
    "                            format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "    logger = logging.getLogger('TFIDF for grant')\n",
    "    handler = logging.FileHandler(args.load_path + \"logfile\")\n",
    "    logger.addHandler(handler)\n",
    "    logger.error('TFIDF for grant')\n",
    "    \n",
    "    try:\n",
    "        # model \n",
    "        tfidf =  TfidfVectorizer(ngram_range = ast.literal_eval(args.ngram_range), \\\n",
    "                                 min_df = args.min_df,\\\n",
    "                                 max_features = args.max_features)\n",
    "        # train, valid and test \n",
    "        rfas, pubs, mix_df, \\\n",
    "        train_idx, valid_idx, test_idx, \\\n",
    "        train_citation, valid_citation, citation, \\\n",
    "        train_mixed, valid_mixed, test_mixed = ut.load_data(args.data_path)\n",
    "        rfa_tfidf, rfa_ids, tfidf = ut.process_rfa_corpus(df = rfas, vectorizer = tfidf, \n",
    "                                                               outpath = args.load_path, \\\n",
    "                                                               load_pretrained = args.load_pretrained)\n",
    "        train_pubs_tfidf, train_pmids = ut.process_pub_query(idx = train_idx, mix_df = mix_df, pubs = pubs, \n",
    "                                                          vectorizer = tfidf, idx_name = 'train_', \\\n",
    "                                                          outpath = args.load_path, \\\n",
    "                                                          load_pretrained = args.load_pretrained)\n",
    "        valid_pubs_tfidf, valid_pmids = ut.process_pub_query(idx = valid_idx, mix_df = mix_df, pubs = pubs, \n",
    "                                                          vectorizer = tfidf, idx_name = 'valid_', \\\n",
    "                                                          outpath = args.load_path, \\\n",
    "                                                          load_pretrained = args.load_pretrained)\n",
    "        test_pubs_tfidf, test_pmids = ut.process_pub_query(idx = test_idx, mix_df = mix_df, pubs = pubs, \n",
    "                                                          vectorizer = tfidf, idx_name = 'test_', \\\n",
    "                                                          outpath = args.load_path, \\\n",
    "                                                          load_pretrained = args.load_pretrained)\n",
    "        # prediction \n",
    "        train_dict = ut.sim_recommend(corpus_vecs = rfa_tfidf, corpus_ids = rfa_ids,\\\n",
    "                           query_vecs = train_pubs_tfidf, query_ids = train_pmids, \n",
    "                           mix_dict = train_mixed, mode= 'strict', outpath = args.load_path, query_name = 'train_', \n",
    "                           top = args.top)\n",
    "        valid_dict = ut.sim_recommend(corpus_vecs = rfa_tfidf, corpus_ids = rfa_ids,\\\n",
    "                           query_vecs = valid_pubs_tfidf, query_ids = valid_pmids, \n",
    "                           mix_dict = valid_mixed, mode= 'strict', outpath = args.load_path, query_name = 'valid_', \n",
    "                           top = args.top)\n",
    "        test_dict = ut.sim_recommend(corpus_vecs = rfa_tfidf, corpus_ids = rfa_ids,\\\n",
    "                           query_vecs = test_pubs_tfidf, query_ids = test_pmids, \n",
    "                           mix_dict = test_mixed, mode= 'strict', outpath = args.load_path, query_name = 'test_', \n",
    "                           top = args.top)\n",
    "        # evaluation on train and test \n",
    "        logger.error('=======train statistics======')\n",
    "        ut.print_metrics(citation = train_citation, similarity_dict = train_dict, logger = logger, ks = [1, 5])\n",
    "        print('=========================================')\n",
    "        logger.error('=======test statistics======')\n",
    "        ut.print_metrics(citation = citation, similarity_dict = test_dict, logger = logger, ks = [1, 5])\n",
    "        logging.shutdown()\n",
    "        for handler in logger.handlers:\n",
    "            if isinstance(handler, logging.FileHandler):\n",
    "                handler.close()\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(colored('--' * 70, 'green'))\n",
    "        print(colored('Exiting from training early', 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR:\n",
      "0.8648628804436893\n",
      "recall@1, recall@5:\n",
      "0.6871173657727649\n",
      "0.8842527913131382\n",
      "precision@1, precision@5:\n",
      "0.7454027101009757\n",
      "0.6933972050900526\n",
      "MAP:\n",
      "0.8648628804436893\n",
      "=========================================\n",
      "MRR:\n",
      "0.8646486758507894\n",
      "recall@1, recall@5:\n",
      "0.6878090205699189\n",
      "0.8853745989809398\n",
      "precision@1, precision@5:\n",
      "0.7454991507831666\n",
      "0.6932515568975278\n",
      "MAP:\n",
      "0.8646486758507894\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
