{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive summary\n",
    "\n",
    "* this notebook contains the experimentations in the publication  \"a novel NIH research grant recommender using BERT\"\n",
    "* major experimentes conducted\n",
    " * creating all testing data, using class from dataProcessing_bert_service, with borrowed data processing from CVProcessing\n",
    " * predictions with already trained model (load trained, get results and write results from utilities from utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f0eec285ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#general \n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import dill\n",
    "import logging\n",
    "\n",
    "#you cannot live without \n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "import random\n",
    "from termcolor import colored\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "#pip install transformers\n",
    "#pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#bert related\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#self-defined\n",
    "from dataProcessing_bert_service import RFADataProcessForPred as DP\n",
    "import utils_bert_service as ut\n",
    "from utils_bert_service import flat_accuracy,flat_accuracy, format_time, set_seed, train_batch, evaluate_batch, save_model, plot_train\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from rfa_bert import GrantModel\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for calling the file from terminal \n",
    "#in the order that they will be used\n",
    "parser = argparse.ArgumentParser(description = 'BERT model for Grant recommendation, service mode')\n",
    "parser.add_argument('-data_path', type = str, default = '../newdata/', \n",
    "                    help = 'complete path to the rfa data  [default:newdata/]')\n",
    "parser.add_argument('-cv_path', type = str, default = '../../part2_CollaRec/service/', \n",
    "                    help = 'complete path to users CV  [default:../../part2_CollaRec/service/, \\\n",
    "                    used initially in collaborator recommendation]')\n",
    "parser.add_argument('-load_pretrained', type = bool, default = True,\n",
    "                    help = 'wehther to load pretrained bert embeddings & tokenizer [default:False]')\n",
    "parser.add_argument('-load_path', type = str, default = '../model_uq/', \n",
    "                    help = \"\"\"path where fine-tuned bert embeddings & tokenizer  \n",
    "                           are saved [default:../model_uq/, normally should be evalAuto/bert/ \n",
    "                           but we are borrowing from trained from other projct: uncertainty quantification]\"\"\")\n",
    "parser.add_argument('-cuda_device', type = int, default = 1, \n",
    "                    help = 'if has cuda, device number to be used [default:1]')\n",
    "#user name\n",
    "parser.add_argument('-f_name', type = str, default = 'Bijal', \n",
    "                    help = \"first name of the reseacher, sentence captilization\")\n",
    "parser.add_argument('-m_name', type = str, default = 'A', \n",
    "                    help = \"middle name of the reseacher, sentence captilization\")\n",
    "parser.add_argument('-l_name', type = str, default = 'Balasubramanian', \n",
    "                    help = \"last name of the reseacher, sentence captilization\")\n",
    "parser.add_argument('-top', type = int, default = 20, \n",
    "                    help = 'number of recommendations (per cluster) [default:20]')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    seed_val = 1234\n",
    "    set_seed(seed_val) \n",
    "    \n",
    "    # get logger started\n",
    "    logging.basicConfig(level=logging.ERROR, filename= \"logfile\", filemode=\"a+\",\n",
    "                            format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "    logger = logging.getLogger('Grant recommender service')\n",
    "    handler = logging.FileHandler(\"logfile\")\n",
    "    logger.addHandler(handler)\n",
    "    logger.error('Grant recommender service')\n",
    "    \n",
    "    try:\n",
    "        dp =  DP(path1 = args.data_path, path2 = args.cv_path,\n",
    "                 load_pretrained = args.load_pretrained, load_path = args.load_path, \n",
    "                 f_name = args.f_name, m_name = args.m_name, l_name = args.l_name,\n",
    "                 logger = logger)\n",
    "        dp.dataframize_()\n",
    "        test_loader, _ = dp.dataloaderize_() \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            use_cuda = torch.device('cuda:' + str(args.cuda_device))\n",
    "        else:\n",
    "            use_cuda = torch.device('cpu')\n",
    "\n",
    "        model = GrantModel(load_pretrained = args.load_pretrained, load_path = args.load_path).model\n",
    "        model.to(use_cuda)\n",
    "        \n",
    "        #predictions \n",
    "        pred_flat, probas = ut.getPredRes(model = model,\n",
    "                         test_loader = test_loader,\n",
    "                         use_cuda = use_cuda,\n",
    "                         f_name = args.f_name,  l_name = args.l_name)\n",
    "        \n",
    "        clustered  = ut.clustered_recom(f_name = args.f_name, m_name = args.m_name, l_name = args.l_name,\n",
    "                     data_path = args.data_path, logger = logger,\n",
    "                     top = args.top)\n",
    "        \n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(colored('--' * 70, 'green'))\n",
    "        print(colored('Exiting from training early', 'green'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs dataframe loaded\n",
      "length of the corpus 235392\n",
      "sample of the corpus ['survey strategies increase participant response rates primary care research studies ', 'survey strategies increase participant response rates primary care research studies ']\n",
      "length of the corpus 235392\n",
      "sample of the corpus ['diabetes research training behavioral scientist t32 foster development diverse highly trained workforce behavioral scientist assume leadership role relate nation research effort area type diabetes national institute diabetes digestive kidney disease niddk national institute nursing research ninr invite application establishment institutional research training program develop cadre diverse highly trained behavioral scientist conduct research relevant improve clinical management quality life patient type diabetes.training grant t32 award make eligible institution provide program prepare predoctoral postdoctoral behavioral scientist select institution behavioral research career type diabetes stage training supervision mentorship include diabetologist behavioral scientist way maximize relevance training type diabetes encourage multi-disciplinary approach research', 'novel innovative tool facilitate identification track manipulation analysis glycans function r21 common fund program accelerate translation glycoscience integration accessibility- aim develop accessible affordable new tool technology study carbohydrate enable researcher biomedical field dramatically advance understanding role complex molecule health disease abandon glycan discovery due difficulty inability study foa solicit development new easily accessible tool reagent technology facilitate identification track manipulation analysis glycans biological binding partner determine function initiative build effort interface exist technology procedure make easy access use applicable effort must consider factor scale-up effort make instrumentation broadly accessible cost-effective end-user compatibility data generate integration exist database']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jzhu/.conda/envs/py37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...DONE.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
