{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ec5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics \n",
    "from os.path import exists\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "# from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# local\n",
    "from prepare_data_bsl import yearly_authors\n",
    "from prepare_dataset_bsl import CollabDataset\n",
    "import utils_bsl as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # define arguments\n",
    "    parser = argparse.ArgumentParser('baseline link predictions')\n",
    "    # data \n",
    "    parser.add_argument( '--data', type=str, help='collab for our own experiments',\n",
    "                    default='collab')\n",
    "    parser.add_argument('--yrs', default = [2010, 2011], type = int, help='years to work on')\n",
    "    parser.add_argument('--authfile', default = '../../DLrec/newdata/processed_pubs.pickle', \\\n",
    "                        help='crawed pubmed database')\n",
    "    parser.add_argument('--inpath', default = '../sage/data/mesh/20102011/', \\\n",
    "                        help=\"since we are using the same dataset as the SAGE, we can reuse the processed dataset\")\n",
    "    parser.add_argument('--node_options', default = 'mesh', \\\n",
    "                        help=\"node feature options, choose from mesh/pubs\")\n",
    "    parser.add_argument('--savepath', type=str, help='path to save the data',\n",
    "                    default='20102011_mesh/')\n",
    "\n",
    "    # model \n",
    "    parser.add_argument('--max_depth', type=int, default=2, help='max depth of the tree') #25/10 for SAGE\n",
    "    parser.add_argument('--lr', type=float, default=0.05, help='Learning rate')\n",
    "    parser.add_argument('--patience_trees', type=int, default= 10, help=' number of trees to wait before stop building')\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='GPU index to use if built trees on GPU')\n",
    "    # training \n",
    "    # parser.add_argument('--n_epoch', type=int, default= 100, help='Number of epochs')\n",
    "    parser.add_argument('--seed', type=int, default=2021, help='One seed that rules them all')\n",
    "    args = parser.parse_args([])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## processing and load data\n",
    "    if not (exists(args.inpath + 'collabs_masks.csv') and exists(args.inpath + 'node_feats.npy')):\n",
    "        yearly_authors(authfile = args.authfile, years = args.yrs, savepath = args.inpath, options = args.node_options) \n",
    "        # dataset processing (graph)\n",
    "        dataset = CollabDataset(raw_dir = args.inpath)\n",
    "    \n",
    "    # original data\n",
    "    df = pd.read_csv(args.inpath + 'collabs_masks.csv')\n",
    "    node_feats = np.load(args.inpath + 'node_feats.npy')\n",
    "    \n",
    "    logger = ut.create_log(args)\n",
    "    \n",
    "    # data preparations; split, get negatives, and merge\n",
    "    df_train, df_val, df_test = ut.split_mask(df, masks = ['train_mask', 'val_mask', 'test_mask']) \n",
    "    \n",
    "    neg_src, neg_dst = ut.create_negatives(src_list = df['new_author'].to_numpy(), \\\n",
    "                                                       dst_list = df['new_coauthor'].to_numpy(), \\\n",
    "                                                       size = len(df))\n",
    "    neg_train_src, neg_train_dst = neg_src[:len(df_train)], neg_dst[:len(df_train)]\n",
    "    neg_val_src, neg_val_dst = neg_src[len(df_train):len(df_train)+ len(df_val)], \\\n",
    "                                neg_dst[len(df_train):len(df_train)+ len(df_val)]\n",
    "    neg_test_src, neg_test_dst = neg_src[-len(df_test):], neg_dst[-len(df_test):]\n",
    "    pos_train_src, pos_train_dst = df_train['new_author'].to_numpy(),  df_train['new_coauthor'].to_numpy()\n",
    "    pos_val_src, pos_val_dst = df_val['new_author'].to_numpy(),  df_val['new_coauthor'].to_numpy()\n",
    "    pos_test_src, pos_test_dst = df_test['new_author'].to_numpy(),  df_test['new_coauthor'].to_numpy()\n",
    "    \n",
    "    # merge first then get the features in numpy\n",
    "    merged_train_src, y_train, merged_train_dst = ut.merge_pn(pos_ls = pos_train_src, neg_ls = neg_train_src, \\\n",
    "                                                             pos_ls2 = pos_train_dst, neg_ls2 = neg_train_dst, \\\n",
    "                                                             seed = args.seed)\n",
    "    merged_val_src, y_val, merged_val_dst = ut.merge_pn(pos_ls = pos_val_src, neg_ls = neg_val_src, \\\n",
    "                                                        pos_ls2 = pos_val_dst, neg_ls2 = neg_val_dst,\n",
    "                                                        seed = args.seed +1 )\n",
    "    merged_test_src, y_test, merged_test_dst = ut.merge_pn(pos_ls = pos_test_src, neg_ls =neg_test_src, \\\n",
    "                                                           pos_ls2 = pos_test_dst, neg_ls2 = neg_test_dst,\n",
    "                                                           seed = args.seed +2)\n",
    "    \n",
    "    X_train = ut.get_features(merged_train_src, feat_np = node_feats, author_ls2 = merged_train_dst)\n",
    "    X_val = ut.get_features(merged_val_src, feat_np = node_feats, author_ls2 = merged_val_dst)\n",
    "    X_test = ut.get_features(merged_test_src, feat_np = node_feats, author_ls2 = merged_test_dst)\n",
    "    \n",
    "    # train & pred\n",
    "    clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                        eval_metric = ['logloss', 'auc'],\n",
    "                        use_label_encoder=False,\n",
    "                        verbosity = 0,\n",
    "                        early_stopping_rounds = args.patience_trees,\n",
    "                        max_depth = args.max_depth, \n",
    "                        learning_rate = args.lr, \n",
    "                        gpu_id = args.gpu, \n",
    "                        random_state = args.seed)\n",
    "\n",
    "    clf = ut.easy_train(logger = logger, clf = clf, \\\n",
    "                        X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val, \\\n",
    "                        path = args.savepath)\n",
    "    ut.easy_predictions(logger = logger, clf = clf, X_test = X_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a9de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
