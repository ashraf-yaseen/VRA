{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality Summary\n",
    "In this notebook, we did (all cleaned):\n",
    "* import: data processing, with split on publications for training, test and validation if split is intended, or else it's not split for prediction \n",
    "* import: utils file for functions definitions \n",
    "* import: model file for model loading\n",
    "* main file (this file) for the whole process of training, validation and/or predictions(can call from both terminal and inside jupyter)\n",
    "\n",
    "**Initially to update everything before feeding to this model**:\n",
    "1. run dataPreprocess.new_collectGEOSummaryfromWeb.py by passing exsiting pickle file & ids needed to be collect to get geo data first\n",
    "2. run dataPreprocess.dataPreProcess.py to get a list of geoIds and corresponding citations info of publications, put them in a list and create true and false pairs\n",
    "3. run dataPreprocess.pubmef_web_parser to get the corresponding publication details, or add ones not in the current 'pub_dataset.pickle' yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general \n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "#you cannot live without \n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "import random\n",
    "from termcolor import colored\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#pip install transformers\n",
    "#pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#bert related\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#self-defined\n",
    "from dataProcessing_bert import DataProcess\n",
    "import utils_bert as ut \n",
    "from clfbert import clfModel\n",
    "from eval_metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newly created dataframe here:\n",
      "\n",
      "       pmid  dataid  match\n",
      "0  17631952    SDY1      1\n",
      "1  16387596    SDY1      1\n",
      "2  21762972   SDY10      1\n",
      "3  20109744   SDY10      1\n",
      "4  23071818  SDY100      1\n",
      "(122135, 3)\n",
      "checking whether all the pairs have information in pubs file:\n",
      "\n",
      "False\n",
      "48929\n",
      "checking whether all the pairs have information in geo file:\n",
      "\n",
      "True\n",
      "61800\n",
      "take subset of pairs whose info are available\n",
      "final screened total pairs and shape:\n",
      "\n",
      "(122111, 3)\n",
      "true pairs total:\n",
      "\n",
      "2380\n",
      "length of the corpus 122111\n",
      "sample of the corpus ['Caspase-12 controls West Nile virus infection via the viral RNA receptor RIG-I. Caspase-12 has been shown to negatively modulate inflammasome signaling during bacterial infection. Its function in viral immunity, however, has not been characterized. We now report an important role for caspase-12 in controlling viral infection via the pattern-recognition receptor RIG-I. After challenge with West Nile virus (WNV), caspase-12-deficient mice had greater mortality, higher viral burden and defective type I interferon response compared with those of challenged wild-type mice. In vitro studies of primary neurons and mouse embryonic fibroblasts showed that caspase-12 positively modulated the production of type I interferon by regulating E3 ubiquitin ligase TRIM25-mediated ubiquitination of RIG-I, a critical signaling event for the type I interferon response to WNV and other important viral pathogens.', 'Genome-wide view of TGFβ/Foxh1 regulation of the early mesendoderm program. Nodal/TGFβ signaling regulates diverse biological responses. By combining RNA-seq on Foxh1 and Nodal signaling loss-of-function embryos with ChIP-seq of Foxh1 and Smad2/3, we report a comprehensive genome-wide interaction between Foxh1 and Smad2/3 in mediating Nodal signaling during vertebrate mesendoderm development. This study significantly increases the total number of Nodal target genes regulated by Foxh1 and Smad2/3, and reinforces the notion that Foxh1-Smad2/3-mediated Nodal signaling directly coordinates the expression of a cohort of genes involved in the control of gene transcription, signaling pathway modulation and tissue morphogenesis during gastrulation. We also show that Foxh1 may function independently of Nodal signaling, in addition to its role as a transcription factor mediating Nodal signaling via Smad2/3. Finally, we propose an evolutionarily conserved interaction between Foxh1 and PouV, a mechanism observed in Pou5f1-mediated regulation of pluripotency in human embryonic stem and epiblast cells.']\n",
      "length of the corpus 122111\n",
      "sample of the corpus ['A Mouse Model of Chronic West Nile Virus Using the Collaborative Cross, a population of recombinant inbred mouse strains with high levels of standing genetic variation, we have identified a mouse model of persistent WNV disease, with persistence of viral loads within the brain.', 'Genome-wide DNA methylation analysis reveals dynamic changes in the cardiac methylome during post-natal heart development (RNA-Seq) nan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jzhu/.conda/envs/py37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "def main():    \n",
    "        \n",
    "    #for calling the file from terminal \n",
    "    parser = argparse.ArgumentParser(description = 'BERT model for data to paper recommendation')\n",
    "    #do aruguments here when not calling from terminal/inside jupyter notebook \n",
    "    args = parser.parse_args([])\n",
    "    args.data_path1 = 'data/'\n",
    "    args.data_path2 = 'IIdata/'\n",
    "    args.subpath = 'sensitivity1vs0.1/'\n",
    "    args.load_pretrained = False\n",
    "    args.load_path= 'model_save_v5_sensitivity1vs0.1/'\n",
    "    args.split1 = True\n",
    "    args.newSplit1= True\n",
    "    args.split2 = False\n",
    "    args.newSplit2 = False \n",
    "\n",
    "    \n",
    "    args.cuda_device = 1\n",
    "    args.learning_rate = 2e-5\n",
    "    args.epsilon = 1e-8\n",
    "    args.train_epochs = 4 \n",
    "    args.plot_train = True\n",
    "    args.names1 = []\n",
    "    args.names2 = ['immport', 'imspace', 'itnshare','geo','srastudies']\n",
    "    args.train_ratio = 0.1\n",
    "    \n",
    "    #make sure results are replicable\n",
    "    seed_val = 1234\n",
    "    ut.set_seed(seed_val)\n",
    "    \n",
    "    #load dataloader\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #load dataloader\n",
    "    dp2 =  DataProcess(path= args.data_path2,\n",
    "                       subpath = args.subpath,\n",
    "          load_pretrained = args.load_pretrained, \n",
    "          load_path = args.load_path,\n",
    "          split = args.split1,\n",
    "          newSplit = args.newSplit1,\n",
    "          names = args.names2,\n",
    "          train_ratio = args.train_ratio)\n",
    "    dp2.dataframize_()\n",
    "    train_loader, _, valid_loader, test_loader = dp2.dataloaderize_() #dataloader right here, len of records 83512, 10816, 25016 \n",
    "    \n",
    "    print(len(train_loader), len(valid_loader), len(test_loader))\n",
    "    #check device\n",
    "    if torch.cuda.is_available():\n",
    "        use_cuda = torch.device('cuda:' + str(args.cuda_device))\n",
    "    else:\n",
    "        use_cuda = torch.device('cpu')\n",
    "        \n",
    "    #load model for bert \n",
    "    model = clfModel(load_pretrained = args.load_pretrained, load_path = args.load_path).model\n",
    "    model.to(use_cuda)\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\" \n",
    "    some sanity check for debugging, can be ignored\n",
    "    print(len(train_loader)* dp.batch_size, len(valid_loader)*dp.batch_size, len(test_loader)*dp.batch_size)\n",
    "    print(dp.df.iloc[dp.train_idx,:].pmid.nunique())\n",
    "    print(dp.df.iloc[dp.valid_idx,:].pmid.nunique())\n",
    "    print(dp.df.iloc[dp.test_idx,:].pmid.nunique())\n",
    "    \"\"\"\n",
    "\n",
    "    #optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = args.learning_rate,\n",
    "                      eps = args.epsilon)\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    total_steps = len(train_loader) * args.train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    \n",
    "    #train and valid \n",
    "    training_stats = ut.train(epochs = args.train_epochs, \n",
    "                                     model = model,\n",
    "                                     train_loader = train_loader, \n",
    "                                     valid_loader = valid_loader, \n",
    "                                     optimizer = optimizer, \n",
    "                                     scheduler = scheduler, \n",
    "                                     use_cuda = use_cuda,\n",
    "                                     args = args)\n",
    "    \n",
    "    #plot\n",
    "    if args.plot_train:\n",
    "        ut.plot_train(training_stats, args.load_path)\n",
    "        \n",
    "            \n",
    "    #prediction on test\n",
    "    combine_predictions, combine_true_labels = ut.predictions(model = model, \n",
    "                                                              test_loader = test_loader, \n",
    "                                                              use_cuda = use_cuda, \n",
    "                                                              path = args.load_path)\n",
    "    \n",
    "    citation_df = dp2.df.iloc[dp2.test_idx,:]\n",
    "    similarity_dict, max_leng = ut.create_smilarity_dict(citation_df = citation_df, \n",
    "                                                         combine_predictions = combine_predictions, \n",
    "                                                        save_path = args.load_path)\n",
    "    print(max_leng)\n",
    "    #metrics\n",
    "    print('MRR:')\n",
    "    print(Metrics(dp2.citation, leng = max_leng).calculate_mrr(similarity_dict)) #mrr\n",
    "\n",
    "    print('recall@1, recall@10:')\n",
    "    print(Metrics(dp2.citation, leng = max_leng).calculate_recall_at_k(similarity_dict, 1))\n",
    "    print(Metrics(dp2.citation, leng = max_leng).calculate_recall_at_k(similarity_dict, 10))\n",
    "\n",
    "    print('precision@1, precision@10:')\n",
    "    print(Metrics(dp2.citation,leng = max_leng).calculate_precision_at_k(similarity_dict, 1))        \n",
    "    print(Metrics(dp2.citation,leng = max_leng).calculate_precision_at_k(similarity_dict, 10))\n",
    "\n",
    "    print('MAP:')\n",
    "    print(Metrics(dp2.citation,leng = max_leng).calculate_MAP_at_k(similarity_dict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
