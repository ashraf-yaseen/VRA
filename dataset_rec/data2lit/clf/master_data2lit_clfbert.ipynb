{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f67b841b940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#general \n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "#you cannot live without \n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#pip install transformers\n",
    "#pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#bert related\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification,  \\\n",
    "AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "# self defined\n",
    "from dataProcessing import GEOLiTDataProcess as GEOLit\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the corpus 152066\n",
      "sample of the corpus [\"impaired natural killer cell self-education `` missing-self '' responses ly49-deficient mice ly49-mediated recognition mhc-i molecules host cells considered vital natural killer nk -cell regulation education however gene-deficient animal models lacking difficulty deleting large multigene family describe nk gene complex knockdown nkc kd mice lack expression ly49 related mhc-i receptors nk cells nkc kd nk cells exhibit defective killing mhc-i-deficient otherwise normal target cells resulting defective rejection nkc kd mice transplants various types mhc-i-deficient mice self-mhc-i immunosurveillance nk cells nkc kd mice rescued self-mhc-i-specific ly49 transgenes although nkc kd mice display defective recognition mhc-i-deficient tumor cells resulting decreased vivo tumor cell clearance nkg2d- antibody-dependent cell-mediated cytotoxicity-induced tumor cell cytotoxicity cytokine production induced activation receptors efficient ly49-deficient nk cells suggesting mhc-i education nk cells single facet regulating total potential provide direct genetic evidence ly49 expression necessary nk-cell education self-mhc-i molecules absence receptors leads loss mhc-i-dependent `` missing-self '' immunosurveillance nk cells\", 'analysis early c2c12 myogenesis identifies stably differentially expressed transcriptional regulators whose knock-down inhibits myoblast differentiation myogenesis tightly controlled process involving transcriptional activation repression thousands genes although many components transcriptional network regulating later phases myogenesis identified relatively studies described transcriptional landscape first 24 myoblasts commit differentiate dense temporal profiling differentiating c2c12 myoblasts identify 193 transcriptional regulators trs whose expression significantly altered within first 24 myogenesis high-content shrna screen 77 trs involving 427 stable lines identified 42 genes whose knockdown significantly inhibits differentiation c2c12 myoblasts trs differentially expressed within first 24 half inhibited differentiation knocked including known regulators myogenesis myod1 myog myf5 well 19 trs previously associated process surprisingly similar proportion 55 shrnas targeting trs whose expression change also inhibited c2c12 myogenesis show subset trs inhibits myogenesis downregulating expression known regulatory structural proteins findings clearly illustrate several trs critical c2c12 myogenesis differentially regulated suggesting approaches focus functional studies differentially-expressed transcripts fail provide comprehensive view complex process']\n",
      "length of the corpus 152066\n",
      "sample of the corpus [\"impaired natural killer cell self-education 'missing-self responses ly49-deficient mice nimblegen arrays used perform acgh analysis mice strains one lacks ly49 gene cluster parent strain still cluster targeted knock-out cluster resulted rearrangment deletion cluster acgh experiment performed elucidate extent deletion\", 'expression profiling early myogenesis illumina dataset myogenesis tightly controlled process involving transcriptional activation repression thousands genes although many components transcriptional network known later phases myogenesis relatively little work described transcriptional landscape within first 24 hours myoblasts commit differentiate dense temporal sampling differentiating c2c12 myoblasts identify 266 transcriptional regulators trs whose expression altered within first 12 hours myogenesis high-content shrna screen 76 trs involving 427 stable lines identified 48 genes whose knockdown significantly inhibits differentiation c2c12 myoblasts include known regulators myogenesis myod1 myog myf5 well 26 regulators previously associated process trs differentially expressed within first 24 hours two-thirds inhibited differentiation knocked surprisingly similar proportion 67 shrnas targeting trs whose expression change differentiation also inhibited myogenesis suggesting stably differentially expressed trs essential complex differentiation program implies microarray-based approaches concentrate functional validation studies differentially-expressed genes fail identify many genes critically implicated complex biological processes']\n"
     ]
    }
   ],
   "source": [
    "geolit = GEOLit()\n",
    "#then to dataloader \n",
    "train_loader, valid_loader, test_loader = geolit.dataloaderize_() #dataloader right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.set_device(1)\n",
    "#print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda118b2588749ea920a1160d0c8fbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef6b7da8fa64466825802902b3df7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = True # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch   200  of  6,653.    Elapsed: 0:01:28.\n",
      "  Batch   400  of  6,653.    Elapsed: 0:02:57.\n",
      "  Batch   600  of  6,653.    Elapsed: 0:04:25.\n",
      "  Batch   800  of  6,653.    Elapsed: 0:05:53.\n",
      "  Batch 1,000  of  6,653.    Elapsed: 0:07:21.\n",
      "  Batch 1,200  of  6,653.    Elapsed: 0:08:49.\n",
      "  Batch 1,400  of  6,653.    Elapsed: 0:10:18.\n",
      "  Batch 1,600  of  6,653.    Elapsed: 0:11:46.\n",
      "  Batch 1,800  of  6,653.    Elapsed: 0:13:15.\n",
      "  Batch 2,000  of  6,653.    Elapsed: 0:14:43.\n",
      "  Batch 2,200  of  6,653.    Elapsed: 0:16:12.\n",
      "  Batch 2,400  of  6,653.    Elapsed: 0:17:40.\n",
      "  Batch 2,600  of  6,653.    Elapsed: 0:19:08.\n",
      "  Batch 2,800  of  6,653.    Elapsed: 0:20:36.\n",
      "  Batch 3,000  of  6,653.    Elapsed: 0:22:04.\n",
      "  Batch 3,200  of  6,653.    Elapsed: 0:23:31.\n",
      "  Batch 3,400  of  6,653.    Elapsed: 0:24:59.\n",
      "  Batch 3,600  of  6,653.    Elapsed: 0:26:27.\n",
      "  Batch 3,800  of  6,653.    Elapsed: 0:27:55.\n",
      "  Batch 4,000  of  6,653.    Elapsed: 0:29:23.\n",
      "  Batch 4,200  of  6,653.    Elapsed: 0:30:51.\n",
      "  Batch 4,400  of  6,653.    Elapsed: 0:32:19.\n",
      "  Batch 4,600  of  6,653.    Elapsed: 0:33:47.\n",
      "  Batch 4,800  of  6,653.    Elapsed: 0:35:15.\n",
      "  Batch 5,000  of  6,653.    Elapsed: 0:36:42.\n",
      "  Batch 5,200  of  6,653.    Elapsed: 0:38:10.\n",
      "  Batch 5,400  of  6,653.    Elapsed: 0:39:38.\n",
      "  Batch 5,600  of  6,653.    Elapsed: 0:41:06.\n",
      "  Batch 5,800  of  6,653.    Elapsed: 0:42:35.\n",
      "  Batch 6,000  of  6,653.    Elapsed: 0:44:03.\n",
      "  Batch 6,200  of  6,653.    Elapsed: 0:45:31.\n",
      "  Batch 6,400  of  6,653.    Elapsed: 0:46:59.\n",
      "  Batch 6,600  of  6,653.    Elapsed: 0:48:27.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:48:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.07\n",
      "  Validation took: 0:01:05\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch   200  of  6,653.    Elapsed: 0:01:28.\n",
      "  Batch   400  of  6,653.    Elapsed: 0:02:56.\n",
      "  Batch   600  of  6,653.    Elapsed: 0:04:25.\n",
      "  Batch   800  of  6,653.    Elapsed: 0:05:53.\n",
      "  Batch 1,000  of  6,653.    Elapsed: 0:07:21.\n",
      "  Batch 1,200  of  6,653.    Elapsed: 0:08:49.\n",
      "  Batch 1,400  of  6,653.    Elapsed: 0:10:17.\n",
      "  Batch 1,600  of  6,653.    Elapsed: 0:11:45.\n",
      "  Batch 1,800  of  6,653.    Elapsed: 0:13:14.\n",
      "  Batch 2,000  of  6,653.    Elapsed: 0:14:42.\n",
      "  Batch 2,200  of  6,653.    Elapsed: 0:16:10.\n",
      "  Batch 2,400  of  6,653.    Elapsed: 0:17:38.\n",
      "  Batch 2,600  of  6,653.    Elapsed: 0:19:06.\n",
      "  Batch 2,800  of  6,653.    Elapsed: 0:20:34.\n",
      "  Batch 3,000  of  6,653.    Elapsed: 0:22:02.\n",
      "  Batch 3,200  of  6,653.    Elapsed: 0:23:31.\n",
      "  Batch 3,400  of  6,653.    Elapsed: 0:24:59.\n",
      "  Batch 3,600  of  6,653.    Elapsed: 0:26:27.\n",
      "  Batch 3,800  of  6,653.    Elapsed: 0:27:55.\n",
      "  Batch 4,000  of  6,653.    Elapsed: 0:29:23.\n",
      "  Batch 4,200  of  6,653.    Elapsed: 0:30:51.\n",
      "  Batch 4,400  of  6,653.    Elapsed: 0:32:20.\n",
      "  Batch 4,600  of  6,653.    Elapsed: 0:33:48.\n",
      "  Batch 4,800  of  6,653.    Elapsed: 0:35:16.\n",
      "  Batch 5,000  of  6,653.    Elapsed: 0:36:44.\n",
      "  Batch 5,200  of  6,653.    Elapsed: 0:38:12.\n",
      "  Batch 5,400  of  6,653.    Elapsed: 0:39:40.\n",
      "  Batch 5,600  of  6,653.    Elapsed: 0:41:08.\n",
      "  Batch 5,800  of  6,653.    Elapsed: 0:42:37.\n",
      "  Batch 6,000  of  6,653.    Elapsed: 0:44:05.\n",
      "  Batch 6,200  of  6,653.    Elapsed: 0:45:33.\n",
      "  Batch 6,400  of  6,653.    Elapsed: 0:47:01.\n",
      "  Batch 6,600  of  6,653.    Elapsed: 0:48:29.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:48:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.05\n",
      "  Validation took: 0:01:05\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch   200  of  6,653.    Elapsed: 0:01:28.\n",
      "  Batch   400  of  6,653.    Elapsed: 0:02:57.\n",
      "  Batch   600  of  6,653.    Elapsed: 0:04:25.\n",
      "  Batch   800  of  6,653.    Elapsed: 0:05:53.\n",
      "  Batch 1,000  of  6,653.    Elapsed: 0:07:21.\n",
      "  Batch 1,200  of  6,653.    Elapsed: 0:08:49.\n",
      "  Batch 1,400  of  6,653.    Elapsed: 0:10:18.\n",
      "  Batch 1,600  of  6,653.    Elapsed: 0:11:46.\n",
      "  Batch 1,800  of  6,653.    Elapsed: 0:13:14.\n",
      "  Batch 2,000  of  6,653.    Elapsed: 0:14:42.\n",
      "  Batch 2,200  of  6,653.    Elapsed: 0:16:10.\n",
      "  Batch 2,400  of  6,653.    Elapsed: 0:17:38.\n",
      "  Batch 2,600  of  6,653.    Elapsed: 0:19:06.\n",
      "  Batch 2,800  of  6,653.    Elapsed: 0:20:35.\n",
      "  Batch 3,000  of  6,653.    Elapsed: 0:22:03.\n",
      "  Batch 3,200  of  6,653.    Elapsed: 0:23:31.\n",
      "  Batch 3,400  of  6,653.    Elapsed: 0:24:59.\n",
      "  Batch 3,600  of  6,653.    Elapsed: 0:26:28.\n",
      "  Batch 3,800  of  6,653.    Elapsed: 0:27:56.\n",
      "  Batch 4,000  of  6,653.    Elapsed: 0:29:24.\n",
      "  Batch 4,200  of  6,653.    Elapsed: 0:30:52.\n",
      "  Batch 4,400  of  6,653.    Elapsed: 0:32:20.\n",
      "  Batch 4,600  of  6,653.    Elapsed: 0:33:48.\n",
      "  Batch 4,800  of  6,653.    Elapsed: 0:35:16.\n",
      "  Batch 5,000  of  6,653.    Elapsed: 0:36:45.\n",
      "  Batch 5,200  of  6,653.    Elapsed: 0:38:13.\n",
      "  Batch 5,400  of  6,653.    Elapsed: 0:39:41.\n",
      "  Batch 5,600  of  6,653.    Elapsed: 0:41:09.\n",
      "  Batch 5,800  of  6,653.    Elapsed: 0:42:37.\n",
      "  Batch 6,000  of  6,653.    Elapsed: 0:44:05.\n",
      "  Batch 6,200  of  6,653.    Elapsed: 0:45:33.\n",
      "  Batch 6,400  of  6,653.    Elapsed: 0:47:01.\n",
      "  Batch 6,600  of  6,653.    Elapsed: 0:48:29.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:48:52\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.06\n",
      "  Validation took: 0:01:05\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch   200  of  6,653.    Elapsed: 0:01:28.\n",
      "  Batch   400  of  6,653.    Elapsed: 0:02:56.\n",
      "  Batch   600  of  6,653.    Elapsed: 0:04:25.\n",
      "  Batch   800  of  6,653.    Elapsed: 0:05:53.\n",
      "  Batch 1,000  of  6,653.    Elapsed: 0:07:21.\n",
      "  Batch 1,200  of  6,653.    Elapsed: 0:08:50.\n",
      "  Batch 1,400  of  6,653.    Elapsed: 0:10:18.\n",
      "  Batch 1,600  of  6,653.    Elapsed: 0:11:47.\n",
      "  Batch 1,800  of  6,653.    Elapsed: 0:13:15.\n",
      "  Batch 2,000  of  6,653.    Elapsed: 0:14:44.\n",
      "  Batch 2,200  of  6,653.    Elapsed: 0:16:12.\n",
      "  Batch 2,400  of  6,653.    Elapsed: 0:17:40.\n",
      "  Batch 2,600  of  6,653.    Elapsed: 0:19:08.\n",
      "  Batch 2,800  of  6,653.    Elapsed: 0:20:36.\n",
      "  Batch 3,000  of  6,653.    Elapsed: 0:22:04.\n",
      "  Batch 3,200  of  6,653.    Elapsed: 0:23:32.\n",
      "  Batch 3,400  of  6,653.    Elapsed: 0:25:00.\n",
      "  Batch 3,600  of  6,653.    Elapsed: 0:26:28.\n",
      "  Batch 3,800  of  6,653.    Elapsed: 0:27:56.\n",
      "  Batch 4,000  of  6,653.    Elapsed: 0:29:24.\n",
      "  Batch 4,200  of  6,653.    Elapsed: 0:30:51.\n",
      "  Batch 4,400  of  6,653.    Elapsed: 0:32:19.\n",
      "  Batch 4,600  of  6,653.    Elapsed: 0:33:47.\n",
      "  Batch 4,800  of  6,653.    Elapsed: 0:35:15.\n",
      "  Batch 5,000  of  6,653.    Elapsed: 0:36:42.\n",
      "  Batch 5,200  of  6,653.    Elapsed: 0:38:10.\n",
      "  Batch 5,400  of  6,653.    Elapsed: 0:39:38.\n",
      "  Batch 5,600  of  6,653.    Elapsed: 0:41:06.\n",
      "  Batch 5,800  of  6,653.    Elapsed: 0:42:34.\n",
      "  Batch 6,000  of  6,653.    Elapsed: 0:44:01.\n",
      "  Batch 6,200  of  6,653.    Elapsed: 0:45:29.\n",
      "  Batch 6,400  of  6,653.    Elapsed: 0:46:57.\n",
      "  Batch 6,600  of  6,653.    Elapsed: 0:48:25.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:48:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.06\n",
      "  Validation took: 0:01:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 4:18:59 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 200 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].cuda(0)\n",
    "        b_input_mask = batch[1].cuda(0)\n",
    "        b_input_type_ids = batch[2].cuda(0)\n",
    "        b_labels = batch[3].cuda(0)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits , _ = model(b_input_ids, \n",
    "                             token_type_ids=b_input_type_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_loader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in valid_loader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].cuda(0)\n",
    "        b_input_mask = batch[1].cuda(0)\n",
    "        b_input_type_ids = batch[2].cuda(0)\n",
    "        b_labels = batch[-1].cuda(0)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits, _) = model(b_input_ids, \n",
    "                                   token_type_ids=b_input_type_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        #logits = logits.detach().cpu().numpy()\\\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_loader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(valid_loader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106448\n",
      "13696\n"
     ]
    }
   ],
   "source": [
    "print (len(train_loader) *  geolit.batch_size)\n",
    "print (len(valid_loader) *  geolit.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
      "epoch                                                                         \n",
      "1           2.74e-02         0.07           0.99       0:48:51         0:01:05\n",
      "2           1.33e-02         0.05           0.99       0:48:53         0:01:05\n",
      "3           5.15e-03         0.06           0.99       0:48:52         0:01:05\n",
      "4           1.96e-03         0.06           0.99       0:48:48         0:01:05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "print(df_stats)\n",
    "df_stats.to_csv('df_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f666d1c8f98>,\n",
       "  <matplotlib.axis.XTick at 0x7f671c4b8e80>,\n",
       "  <matplotlib.axis.XTick at 0x7f671c4b8ac8>,\n",
       "  <matplotlib.axis.XTick at 0x7f66b46697b8>],\n",
       " <a list of 4 Text xticklabel objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 31,920 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_loader1) *  geolit.batch_size ))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict\n",
    "test_len = []\n",
    "for batch in test_loader1:\n",
    "        res = batch[0].size(0) == batch[1].size(0) == batch[2].size(0) ==  batch[-1].size(0)\n",
    "        test_len.append(res)\n",
    "        #print(batch[-1].size())\n",
    "        #break\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids = batch[0].cuda(0)\n",
    "        b_input_mask = batch[1].cuda(0)\n",
    "        b_input_type_ids = batch[2].cuda(0)\n",
    "        b_labels = batch[-1].cuda(0)\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          (loss, logits, _) = model(b_input_ids, \n",
    "                                   token_type_ids=b_input_type_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "\n",
    "\n",
    "            #logits = outputs[0] # 0 or 1?\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "combine_predictions = np.concatenate(predictions, axis=0)\n",
    "# combine correct labels across all batches \n",
    "combine_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the accuracy \n",
    "accu = flat_accuracy(combine_predictions, combine_true_labels)\n",
    "\n",
    "print('Total accuracy: %.3f' % accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model_save/vocab.txt',\n",
       " 'model_save/special_tokens_map.json',\n",
       " 'model_save/added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = 'model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "geolit.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
