{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "import time       \n",
    "import os \n",
    "import operator \n",
    "\n",
    "## torch \n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "## self defined\n",
    "from utils.find_ranking_citation import CitationRanking\n",
    "from utils.write_result import WriteResult\n",
    "from utils.utils import sort_this\n",
    "from utils.configuraiton import Rec_configuration \n",
    "from utils.eval import Metrics\n",
    "from utils.data_loading import DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Master:\n",
    "    def __init__(self, rerank = False):\n",
    "        self.rec_conf = Rec_configuration()\n",
    "        self.article_title, self.article_abstract, self.geo_title, \\\n",
    "        self.geo_summary, self.citation_data = DataLoading().get_all_details() #we should get actual citations\n",
    "        self.auto_rank = CitationRanking(self.citation_data)\n",
    "        self.write_res = WriteResult()\n",
    "        choose = list(self.citation_data.keys())\n",
    "        self.geo_test_data = [key for key in choose if self.citation_data[key] != []] #76,064\n",
    "        #self.rr = ReRankingTitle() # i definitely need to work on this as well (re-ranking title using bert)\n",
    "        self.top_threshold = 10 #sounds nice\n",
    "        self.rerank = rerank\n",
    "        #bert related\n",
    "        self.model_roberta = SentenceTransformer('roberta-base-nli-stsb-mean-tokens') #roberta \n",
    "        #self.model_addr = self.rec_conf.model_path_roberta #subject to change\n",
    "        self.res_addr = self.rec_conf.result_address_roberta #subject to change\n",
    "\n",
    "        \n",
    "    def re_rank_bert(self, geo_sim_dict, article_title, geo_title, pmid_ls): #pmid_ls. should be the complete pmid: similarity score column dimension\n",
    "        assert np.asarray(list(geo_sim_dict.values())).shape[1] == len(pmid_ls) #the # of columns should be equal to total # of pmids\n",
    "        geo_words = list(geo_title.values())#a list of words? I don't need to split for bert\n",
    "        title_words = list(article_title.values())\n",
    "        sim_value = linear_kernel(self.model_bert.encode(geo_words), self.model_bert.encode(title_words))\n",
    "        for i, geo_id in enumerate(geo_sim_dict): #key of the dictionary is the geo-ids\n",
    "            #for each geo_id, adding re-ranking values\n",
    "            geo_sim_dict[geo_id] += sim_value[i, :]\n",
    "        return self.sorting(geo_sim_dict, pmid_ls)\n",
    "    \n",
    "    def sorting(self, geo_sim_dict, pmid_ls):\n",
    "          \n",
    "        '''\n",
    "        for each geo_id, give a list of recommendations \n",
    "        this could be for non-reranking or reranking\n",
    "        '''\n",
    "        similarity_dict = defaultdict()\n",
    "        sim_np = np.asarray(list(geo_sim_dict.values()))\n",
    "        idx_np = np.argsort(-sim_np, axis= 1) #so the big values will be in front \n",
    "        #take on # of top_threshold\n",
    "        idx_np = idx_np[:,:self.top_threshold]\n",
    "        sim_np_taketop = np.take_along_axis(sim_np, idx_np, axis=1) \n",
    "        for i, geo_id in enumerate(geo_sim_dict):\n",
    "            pmid_selected = list(np.take(pmid_ls, idx_np[i]))\n",
    "            selected = dict(zip(pmid_selected, sim_np_taketop[i]))\n",
    "            similarity_dict[geo_id] = selected     \n",
    "        return similarity_dict\n",
    "    \n",
    "    def process(self):\n",
    "        #check paths and check vecs        \n",
    "        pmids, roberta_vecs = None, None #this should be loading something\n",
    "\n",
    "        # This is for loading the pre-trained bert models.\n",
    "        if roberta_vecs is not None and pmids is not None:\n",
    "            print('load trained models and encoded publications vectors')\n",
    "            '''\n",
    "            \n",
    "            '''\n",
    "        else:\n",
    "            print('Building models')\n",
    "            joined_dict_article = {}\n",
    "            for pmid in self.article_title: #for articles, encoding the articles\n",
    "                if (self.article_title[pmid] + self.article_abstract[pmid]).strip() != '':\n",
    "                    joined_dict_article[pmid] = self.article_title[pmid] + ' ' + self.article_abstract[pmid]\n",
    "            #should be some training here, otherwise just do the easiest encodinh\n",
    "            pmids, roberta_vecs = list(joined_dict_article.keys()), self.model_roberta.encode(list(joined_dict_article.values()))\n",
    "            filename =  self.res_addr + 'base/roberta_vecs'\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(self.res_addr + 'base/roberta_vecs', 'wb') as handle:\n",
    "                pickle.dump(roberta_vecs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            filename =  self.res_addr + 'base/pmids'\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)    \n",
    "            with open(self.res_addr + 'base/pmids', 'wb') as handle:\n",
    "                pickle.dump(pmids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        joined_dict_geo = {} # maxium: 76,064\n",
    "        # screening first for those geo_ids with pmids \n",
    "        for geo_id in self.geo_test_data:\n",
    "            print('geo_id')\n",
    "            print(geo_id)\n",
    "            if (self.geo_title[geo_id] + self.geo_summary[geo_id]).strip() != '':\n",
    "                #only get the ones with actual texts\n",
    "                joined_dict_geo[geo_id] = self.geo_title[geo_id] + ' ' + self.geo_summary[geo_id]\n",
    "        geo_ids, geo_roberta_vecs = list(joined_dict_geo.keys()), self.model_roberta.encode(list(joined_dict_geo.values()))\n",
    "        filename =  self.res_addr + 'base/geo_roberta_vecs'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        with open(self.res_addr + 'base/geo_roberta_vecs', 'wb') as handle:\n",
    "            pickle.dump(geo_roberta_vecs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        filename =  self.res_addr + 'base/geo_ids'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)     \n",
    "        with open(self.res_addr + 'base/geo_ids', 'wb') as handle:\n",
    "            pickle.dump(geo_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    x = Master(rerank = False)\n",
    "    x.process()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read-in stored vec \n",
    "res_addr = 'results/roberta_plain/base/'\n",
    "with open(res_addr +'roberta_vecs', 'rb') as fp:\n",
    "      roberta_vecs = pickle.load(fp)\n",
    "with open(res_addr +'geo_roberta_vecs', 'rb') as fp:\n",
    "      geo_roberta_vecs = pickle.load(fp)        \n",
    "        \n",
    "with open(res_addr +'pmids', 'rb') as fp:\n",
    "      pmids = pickle.load(fp)\n",
    "with open(res_addr +'geo_ids', 'rb') as fp:\n",
    "      geo_ids = pickle.load(fp)      \n",
    "geo_roberta_vecs = torch.tensor(np.array(geo_roberta_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 64\n",
    "geo_roberta_vecs_data = TensorDataset(geo_roberta_vecs)\n",
    "geo_roberta_vecs_sampler = SequentialSampler(geo_roberta_vecs_data) #sequential here \n",
    "geo_roberta_vecs_dataloader = DataLoader(geo_roberta_vecs_data, sampler=geo_roberta_vecs_sampler, batch_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, citation_data = DataLoading().get_all_details() #we should get actual citations\n",
    "auto_rank = CitationRanking(citation_data)\n",
    "write_res = WriteResult()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for step, batch in enumerate(geo_roberta_vecs_dataloader):\n",
    "    #print(batch)\n",
    "    batch = batch[0].to(device)\n",
    "    print(batch.shape)\n",
    "    geo_ids_batch = geo_ids[step*b_size: (step+1)*b_size]\n",
    "    #geo_batch, geo_ids_batch = batch.cpu().numpy()\n",
    "    similarity_value_dict = dict()\n",
    "    #similarity_scores = linear_kernel(geo_bert_vecs, bert_vecs)\n",
    "    similarity_scores = torch.cdist(batch, roberta_vecs)  \n",
    "        #we need to export this value\n",
    "    np.save(res_addr + 'similarity_scores_batch_' + str(step), similarity_scores.cpu().numpy())\n",
    "    #load this later\n",
    "    #np.load(self.res_addr + 'base/similarity_scores')\n",
    "    print(similarity_scores.shape)\n",
    "    #call the re-ranking or not \n",
    "    for i, geo_id in enumerate(geo_ids_batch): #this is only from testing geo_id\n",
    "        similarity_value_dict[geo_id] =  list(similarity_scores[i].cpu().numpy())\n",
    "    new_similarity_dict =  sorting(geo_sim_dict = similarity_value_dict, pmid_ls = pmids)\n",
    "    new_similarity_dict = dict(new_similarity_dict)\n",
    "    filename =  res_addr + 'new_similarity_dict_batch_' + str(step)\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(res_addr + 'new_similarity_dict_batch_' + str(step), 'wb') as handle:\n",
    "        pickle.dump(new_similarity_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('dict exported!')\n",
    "\n",
    "    for id, geo_id in enumerate(geo_ids_batch): \n",
    "        temp_selected = new_similarity_dict[geo_id]\n",
    "        write_res.write(res_addr  + geo_id + '.txt', temp_selected)\n",
    "        auto_rank.find_citations(geo_id, list(temp_selected.keys())) #keys are a list of pmids\n",
    "\n",
    "a, b, c, d = auto_rank.get_values() #need to modify this \n",
    "print('good geo recommendations = {}, top1 hit geo recommendations = {}, bad geo recommendations = {}, '\n",
    "      'geo without citations = {}'.format(a, b, c, d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#files = glob.glob(res_addr + \"similarity_scores_batch_*\")\n",
    "res_addr = 'results/bert_plain/base/'\n",
    "new_similarity_dict =  {}\n",
    "#print\n",
    "#for file in files:\n",
    "for i in range(step + 1):\n",
    "    with open(res_addr + \"new_similarity_dict_batch_\" + str(i), 'rb') as fp:\n",
    "        dict_batch = pickle.load(fp)\n",
    "        new_similarity_dict.update(dict_batch)\n",
    "#dict.update(dict2)\n",
    "#dict.update(dict3)\n",
    "with open(res_addr + 'new_similarity_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(new_similarity_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRR \n",
    "print('MRR:')\n",
    "print(Metrics(citation_data).calculate_mrr(new_similarity_dict)) #mrr\n",
    "#recall @1 and recall @10\n",
    "print('recall@1, recall@10:')\n",
    "print(Metrics(citation_data).calculate_recall_at_k(new_similarity_dict, 1))\n",
    "print(Metrics(citation_data).calculate_recall_at_k(new_similarity_dict, 10))\n",
    "#Precision@1 and precision10\n",
    "print('precision@1, precision@10:')\n",
    "print(Metrics(citation_data).calculate_precision_at_k(new_similarity_dict, 1))        \n",
    "print(Metrics(citation_data).calculate_precision_at_k(new_similarity_dict, 10))\n",
    "#MAP@10\n",
    "print(Metrics(citation_data).calculate_MAP_at_k(new_similarity_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
